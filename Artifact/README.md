# README

## Getting Started Guide

In the artifact archive file obtained by downloading and unpacking, we provide the **dockerfile** file that can be built, and the build is expected to take no more than 1 hour with good network conditions (as there is software that needs to be downloaded and installed in the docker image). However, due to network limitation issues in my country, I can't test if the tester is able to cross-compile, but I believe it's doable. And I have provided **zip archives of images** built on this platform that can be directly **docker run**.

### Build From Dockerfile

In the directory of the artifact archive file run the command:

```
docker build . -t <artifact-name>:latest
```

Then you'll get the corresponding docker image **\<artifact-name\>:latest**, and you'll run the corresponding container from the image to complete all subsequent experiments. The container's working directory is **/app**.

### Running directly from a Docker image

Unpacking the docker container's tarball and starting the container has the same effect as the previous method.

**Next you will follow the complete step-by-step guidelines given in Instruction to replicate each part of the experiment.**

------

## Instruction

### Reproducible experimental claims

**We provide artifacts that reproduce some of the following experimental results from the paper** (descriptions are given according to the organization of the paper's experiments)

| **Experimental Item** | ACC  |  PD  |  BB  |
| :-------------------: | :--: | :--: | :--: |
| Training Performance  | Yes  | Yes  | Yes  |
|      Correctness      | Yes  | Yes  | Yes  |
|   Generalizability    | Yes  | Yes  |  No  |
|  Tightness-MaxBound   | Yes  | Yes  | Yes  |
|  Tightness-AvgBound   | Yes  | Yes  | Yes  |
|      Efficiency       | Yes  | Yes  | Yes  |
|      Comparisons      | Yes  | Yes  | Yes  |

All the experiments can be run within the docker container generated by the docker image and there are Python scripts can be run to generate the corresponding experimental results. BB This benchmark could not complete the reproduction of *Generalizability* because we did not record and save the verification policy and related parameters during the experiments.

### artifact code structure

In order for you to better run the Python scripts in the projects we provide, we provide you with the organisation of the code.  The reproduction of the experiment is done by two parts, **TrainPerfomace** and **Verify**. You mainly run the Python scripts in them.

- Artifact
  - env
  - Tools
  - TrainPerfomace
    - trainplot
    - run_ACC.py
    - run_BB.py
    - run_PD.py 
  - Verify
    - AbstractTraining
    - Policies
    - AdaptiveCar.py
    - BouncingBall.py
    - Pendulum.py

## Attention

Since the decision units in the paper need to be abstracted in advance and their resulting **r-tree indexing files** are large, we do not provide them in advance. This results in recreating the rtree index file for the corresponding granularity the first time you perform an experiment at a certain granularity, which may take you a bit of time, and the console really starts to output the progress of the experiment after the index file has been created.

### Training Performance Replicating

Here we give the experimental steps for reproducing **Training Performance** for each benchmark.

From the docker container's interactive command line, you go to the /**app/Verify/TrainingPerformance** directory. The experiments for the three benchmarks are done by the scripts run_ACC.py/run_BB.py/run_PD.py respectively. You will complete the reproduction by running the following command directly.

```
python run_{benchmark}.py
```

The results of the experiments will be saved in the **/trainplot** directory in the Training Performance directory, named ‘benchmark.svg.

### Correctness

#### **ACC** 

Go to the **/app/Verify/Verify** directory and run 

```
python AdaptiveCar.py --Experiment="corr"
```

 on the command line to reproduce the results of the **Correctness** experiment. The experiment will output the verified **Max-Bound** as well as **Avg-Bound**, and the simulated **Max** and **Avg** values. And generate heatmaps consistent with the paper to   **. /plots/ACC/Correctness**, named ‘ACC-Verification.pdf’ and ‘ACC-Simulation.pdf’ respectively.

#### **PD** 

The verification process for BB is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python Pendulum.py --Experiment="corr"
```

 on the command line to reproduce the results of the **Correctness** experiment. The experiment will output the verified **Max-Bound** as well as **Avg-Bound**, and the simulated **Max** and **Avg** values. And generate heatmaps consistent with the paper to **. /plots/PD/Correctness**, named ‘PD-Verification.pdf’ and ‘PD-Simulation.pdf’ respectively.

#### BB

The verification process for PD is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python BouncingBall.py --Experiment="corr"
```

 on the command line to reproduce the results of the **Correctness** experiment. The experiment will output the verified **Max-Bound** as well as **Avg-Bound**, and the simulated **Max** and **Avg** values. And generate heatmaps consistent with the paper to   **. /plots/BB/Correctness**, named ‘BB-Verification.pdf’ and ‘BB-Simulation.pdf’ respectively.

### Generalizability

#### ACC

Go to the **/app/Verify/Verify** directory and run 

```
python AdaptiveCar.py --Experiment="gen"
```

 on the command line to reproduce the results of the **Generalizability** experiment. Includes **Avg-Bound** and **Max-Bound** under <u>different starting regions</u>, <u>different verification horizons</u>, and <u>different policies</u> and 9 heatmaps identical to the corresponding chapters in the paper, respectively. **Area_i.pdf**, **Horizon_k.pdf**, {policy}.pdf under **./plots/ACC/Generalizability**.

#### PD

Go to the **/app/Verify/Verify** directory and run 

```
python Pendulum.py --Experiment="gen"
```

 on the command line to reproduce the results consistent with the results in the **appendix**. Includes **Avg-Bound** and **Max-Bound** under <u>different starting regions</u>, <u>different verification horizons</u>, and <u>different policies</u> and 9 heatmaps identical to the corresponding chapters in the paper, respectively. **Area_i.pdf**, **Horizon_k.pdf**, {policy}.pdf under **./plots/PD/Generalizability**.

#### BB

This experiment for BB could not reproduce the results in the appendix due to the loss of the original experimental files.

### Tightness-MaxBound

#### ACC

Go to the **/app/Verify/Verify** directory and run 

```
python AdaptiveCar.py --Experiment="maxe" --level=i
```

 on the command line to reproduce the results of the **Tightness-MaxBound** experiment. The parameter *--level==i* specifies the level of granularity, *i* can be specified as 1-5.  When the code is run it will output on the console the progress of the verification at the  granularity level and the results **Avg-Bound** and **Max-Bound**, as well as the simulation result **Max**. when the verification is finished, they will be stored in **. /results/ACC/Max_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.

#### PD

The verification process for PD is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python Pendulum.py --Experiment="maxe" --level=i
```

 on the command line to reproduce the results of the **Tightness-MaxBound** experiment. The parameter *--level==i* specifies the level of granularity, *i* can be specified as 1-5.  When the code is run it will output on the console the progress of the verification at the  granularity level and the results **Avg-Bound** and **Max-Bound**, as well as the simulation result **Max**. when the verification is finished, they will be stored in **. /results/PD/Max_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.



#### BB

The verification process for BB is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python BouncingBall.py --Experiment="maxe" --level=i
```

 on the command line to reproduce the results of the **Tightness-MaxBound** experiment. The parameter *--level==i* specifies the level of granularity, *i* can be specified as 1-5.  When the code is run it will output on the console the progress of the verification at the  granularity level and the results **Avg-Bound** and **Max-Bound**, as well as the simulation result **Max**. when the verification is finished, they will be stored in **. /results/BB/Max_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.

### Tightness-AvgBound and Efficiency 

#### ACC

Go to the **/app/Verify/Verify** directory and run 

```
python AdaptiveCar.py --Experiment="avge" --level=i
```

 on the command line to reproduce the results of the **Tightness-AvgBound** experiment. The parameter *--level==i* specifies the level of granularity, *i* can be specified as 1-5.  When the code is run it will output on the console the progress of the verification at the  granularity level and the results **Avg-Bound** as well as the simulation result **Avg**.  The time for verification is mainly for the process of building the MDP, and you can see the elapsed time consumed up to each $k$ from the console output. when the verification is finished, the verification result will be stored in **. /results/ACC/Avg_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.  The Timecost will be stored in another **./results/ACC/Timecost.xlsx** with a corresponding sheet **Level_i**.

#### PD

The verification process for PD is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python Pendulum.py --Experiment="avge" --level=i
```

When the verification is finished, the verification result will be stored in **. /results/PD/Avg_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.  The Timecost will be stored in another **./results/PD/Timecost.xlsx** with a corresponding sheet **Level_i**.

#### BB

The verification process for BB is similar to ACC.

Go to the **/app/Verify/Verify** directory and run 

```
python BouncingBall.py --Experiment="avge" --level=i
```

When the verification is finished, the verification result will be stored in **. /results/BB/Avg_Error.xlsx** to generate a sheets **Level_i** to store the verification and simulation results under each 5 step, and the corresponding error values.  The Timecost will be stored in another **./results/BB/Timecost.xlsx** with a corresponding sheet **Level_i**.

### Comparisons

#### ACC

Go to the **/app/Verify/Verify** directory and run 

```
python AdaptiveCar.py --Experiment="com"
```

 on the command line to reproduce the results of the **Comparisons** experiment. The results obtained contain the data presented in the table in the Comparisons section, i.e., under step $k=7,30,60:$​ the **Time** of verification (obtained from the construction of the MDP) as well as the verified **Avg-Bound**, Max-Bound and simulated **Avg**, Max and the error terms **E-Avg** and **E-Max**.

#### PD

Go to the **/app/Verify/Verify** directory and run 

```
python Pendulum.py --Experiment="com"
```

 on the command line to reproduce the results of the **Comparisons** experiment. The results obtained contain the data presented in the table in the Comparisons section, i.e., under step $k=6,150,300:$ the **Time** of verification (obtained from the construction of the MDP) as well as the verified **Avg-Bound**, Max-Bound and simulated **Avg**, Max and the error terms **E-Avg** and **E-Max**.

#### BB

Go to the **/app/Verify/Verify** directory and run 

```
python BouncingBall.py --Experiment="com"
```

 on the command line to reproduce the results of the **Comparisons** experiment. The results obtained contain the data presented in the table in the Comparisons section, i.e., under step $k=20,40,60:$ the **Time** of verification (obtained from the construction of the MDP) as well as the verified **Avg-Bound**, Max-Bound and simulated **Avg**, Max and the error terms **E-Avg** and **E-Max**.

------

## Raw data and logs

We also provide some of the original experimental data from the paper in the archive file for reference. The folder is **RawData**.

where ACC/PD/BB contain data on their respective Tightness-Efficiency. The reproduced results of the other experiments are in the form of pictures, which can be directly compared with the paper.